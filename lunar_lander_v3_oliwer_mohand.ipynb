{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d10fd84-977a-429e-99e0-ef3f3dc29062",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d962ce54-40a8-4aa7-ab57-71219416b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "884fe7ae-aaac-4c39-88cf-fc84083c1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "env_name = 'LunarLander-v3' # Nom de l'environnement\n",
    "epsilon = 1.0 # Valeur initiale d'epsilon (exploration)\n",
    "epsilon_min = 0.01 # Valeur minimale d'epsilon\n",
    "epsilon_decay = 0.99 # Facteur de décroissance d'epsilon\n",
    "gamma = 0.99 # Eviter que ça mange comme un doberman (Entraienment sur le long terme)\n",
    "batch_size = 32 # Taille de l'entraienement (données mémoire)\n",
    "memory_size = 1000000 # Taille de la mémoire \n",
    "episodes = 500 # Nombre d'épisodes d'entraînement (itérations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf848119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, np.int64(4))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environnement\n",
    "env = gym.make(env_name) # Initialisation de l'environnement (LunarLander-v3)\n",
    "state_shape = env.observation_space.shape[0] # Récupération de l'état de l'environnement\n",
    "action_shape = env.action_space.n # Récupération de l'action de l'environnement\n",
    "\n",
    "state_shape, action_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fddfd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "callbacks = [\n",
    "    #ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6),\n",
    "    #EarlyStopping(monitor='loss_value', patience=10, restore_best_weights=True),\n",
    "    TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True),\n",
    "    #ModelCheckpoint(filepath='modelLunar.keras', save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b17b6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du modèle\n",
    "\n",
    "def create_q_model(): \n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(64, input_shape=(state_shape,)),\n",
    "            Dense(64),\n",
    "            Dense(action_shape, activation='linear')\n",
    "        ]\n",
    "    )\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0b765ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des modèles\n",
    "\n",
    "q_model = create_q_model() # Création du modèle\n",
    "target_model = create_q_model() # Création du modèle cible\n",
    "target_model.set_weights(q_model.get_weights()) # Initialisation du modèle cible avec les poids du modèle principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc7b2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de la mémoire\n",
    "\n",
    "memory = deque(maxlen=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b09a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de stockage de la mémoire\n",
    "\n",
    "def store_transition(state, action, reward, next_state, terminated):\n",
    "    memory.append((state, action, reward, next_state, terminated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6767196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Echantillonnage du batch\n",
    "\n",
    "def sample_batch(): \n",
    "    batch = random.sample(memory, batch_size) # Echantillonnage aléatoire du batch\n",
    "    state, action, reward, next_state, terminated = map(np.asarray, zip(*batch))\n",
    "    return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(terminated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86ba3209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-greedy policy\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if np.random.random() < epsilon: # Exploration (Si le nombre aléatoire est inférieur à epsilon, on choisi une action aléatoire)\n",
    "        return np.random.choice(action_shape)\n",
    "    else: # Si non, on utilise le modèle pour prédire l'action\n",
    "        q_values =  q_model.predict(state[np.newaxis], verbose=0) \n",
    "        return np.argmax(q_values[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4826f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement\n",
    "\n",
    "def train_step():\n",
    "    if len(memory) < batch_size: # Si la mémoire est inférieure à la taille du batch, on ne fait rien\n",
    "        return\n",
    "    \n",
    "    state, action, reward, next_state, terminated = sample_batch() # Echantillonnage du batch\n",
    "    \n",
    "    next_q_values = target_model.predict(next_state, verbose=0) # Prédiction des valeurs Q de l'état suivant\n",
    "    max_next_q_values = np.max(next_q_values, axis=1) # Récupération de la valeur maximale des valeurs Q de l'état suivant\n",
    "\n",
    "    target_q_values = q_model.predict(state, verbose=0) # Prédiction des valeurs Q de l'état actuel\n",
    "\n",
    "    for i, act in enumerate(action):\n",
    "        target_q_values[i][act] = reward[i] if terminated[i] else reward[i] + gamma * max_next_q_values[i]\n",
    "\n",
    "    q_model.fit(state, target_q_values, epochs=1, verbose=0, callbacks=callbacks) # Entraînement du modèle sur l'état actuel et les valeurs Q cibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed1eacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history = [] # Historique des récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1b845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500, Total Reward: -107.86061669878193, Epsilon: 0.9900\n",
      "Episode 2/500, Total Reward: -460.4826624378263, Epsilon: 0.9801\n",
      "Episode 3/500, Total Reward: -276.5252598458003, Epsilon: 0.9703\n",
      "Episode 4/500, Total Reward: -96.30748873987928, Epsilon: 0.9606\n",
      "Episode 5/500, Total Reward: -128.06239282383603, Epsilon: 0.9510\n",
      "Episode 6/500, Total Reward: -145.98853284075153, Epsilon: 0.9415\n",
      "Episode 7/500, Total Reward: -201.98847562546302, Epsilon: 0.9321\n",
      "Episode 8/500, Total Reward: -90.33368952506099, Epsilon: 0.9227\n",
      "Episode 9/500, Total Reward: -478.32259998377856, Epsilon: 0.9135\n"
     ]
    }
   ],
   "source": [
    "# Itération des entrainements\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    terminated = False\n",
    "    \n",
    "    while not terminated:\n",
    "        action = epsilon_greedy_policy(state, epsilon) # Sélection de l'action avec la politique epsilon-greedy\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action) # Modifier l'environnement avec l'action\n",
    "        terminated = terminated or truncated # Vérification de la terminaison de l'épisode\n",
    "        store_transition(state, action, reward, next_state, terminated) # Stockage de la transition dans la mémoire\n",
    "        total_reward += reward # Ajout de la récompense totale\n",
    "\n",
    "        state = next_state # Mise à jour de l'état\n",
    "        train_step() # Entraînement du modèle\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay) # Décroissance d'epsilon (réduction de l'exploration)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        target_model.set_weights(q_model.get_weights()) # Mise à jour du modèle cible avec les poids du modèle principal\n",
    "    \n",
    "    reward_history.append(total_reward) # Ajout de la récompense totale à l'historique des récompenses\n",
    "    print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrement du modèle\n",
    "q_model.save(\"./models/LunarLander_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
