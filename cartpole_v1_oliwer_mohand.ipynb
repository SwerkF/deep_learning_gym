{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b1b2a8",
   "metadata": {},
   "source": [
    "Oliwer & Mohand - CartPole V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f74e4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "385e40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "env_name = 'CartPole-v1' # Nom de l'environnement\n",
    "epsilon = 1.0 # Valeur initiale d'epsilon (exploration)\n",
    "epsilon_min = 0.01 # Valeur minimale d'epsilon\n",
    "epsilon_decay = 0.99 # Facteur de décroissance d'epsilon\n",
    "gamma = 0.99 # Eviter que ça mange comme un doberman (Entraienment sur le long terme)\n",
    "batch_size = 32 # Taille de l'entraienement (données mémoire)\n",
    "memory_size = 1000000 # Taille de la mémoire \n",
    "episodes = 500 # Nombre d'épisodes d'entraînement (itérations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1594d825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, np.int64(2))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environnement\n",
    "env = gym.make(env_name) # Initialisation de l'environnement (Pendulum-v1)\n",
    "state_shape = env.observation_space.shape[0] # Récupération de l'état de l'environnement\n",
    "action_shape = env.action_space.n # Récupération de l'action de l'environnement\n",
    "\n",
    "state_shape, action_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4db35c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "callbacks = [\n",
    "    #ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6),\n",
    "    #EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True),\n",
    "    #ModelCheckpoint(filepath='modelSigmoid.keras', save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef5929a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du modèle\n",
    "\n",
    "def create_q_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, input_shape=(state_shape,), activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(action_shape, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f80943a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des modèles\n",
    "\n",
    "q_model = create_q_model() # Création du modèle\n",
    "target_model = create_q_model() # Création du modèle cible\n",
    "target_model.set_weights(q_model.get_weights()) # Initialisation du modèle cible avec les poids du modèle principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d881096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de la mémoire\n",
    "\n",
    "memory = deque(maxlen=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45931b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de stockage de la mémoire\n",
    "\n",
    "def store_transition(state, action, reward, next_state, terminated):\n",
    "    memory.append((state, action, reward, next_state, terminated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de9e5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Echantillonnage du batch\n",
    "\n",
    "def sample_batch(): \n",
    "    batch = random.sample(memory, batch_size) # Echantillonnage aléatoire du batch\n",
    "    state, action, reward, next_state, terminated = map(np.asarray, zip(*batch))\n",
    "    return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(terminated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ca7bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-greedy policy\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if np.random.random() < epsilon: # Exploration (Si le nombre aléatoire est inférieur à epsilon, on choisi une action aléatoire)\n",
    "        return np.random.choice(action_shape)\n",
    "    else: # Si non, on utilise le modèle pour prédire l'action\n",
    "        q_values =  q_model.predict(state[np.newaxis], verbose=0) \n",
    "        return np.argmax(q_values[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e23995bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement\n",
    "\n",
    "def train_step():\n",
    "    if len(memory) < batch_size: # Si la mémoire est inférieure à la taille du batch, on ne fait rien\n",
    "        return\n",
    "    \n",
    "    state, action, reward, next_state, terminated = sample_batch() # Echantillonnage du batch\n",
    "    \n",
    "    next_q_values = target_model.predict(next_state, verbose=0) # Prédiction des valeurs Q de l'état suivant\n",
    "    max_next_q_values = np.max(next_q_values, axis=1) # Récupération de la valeur maximale des valeurs Q de l'état suivant\n",
    "\n",
    "    target_q_values = q_model.predict(state, verbose=0) # Prédiction des valeurs Q de l'état actuel\n",
    "\n",
    "    for i, act in enumerate(action):\n",
    "        target_q_values[i][act] = reward[i] if terminated[i] else reward[i] + gamma * max_next_q_values[i]\n",
    "\n",
    "    q_model.fit(state, target_q_values, epochs=1, verbose=0, callbacks=callbacks) # Entraînement du modèle sur l'état actuel et les valeurs Q cibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "081cb95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history = [] # Historique des récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3ef67d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500, Total Reward: 40.0, Epsilon: 0.9900\n",
      "Episode 2/500, Total Reward: 15.0, Epsilon: 0.9801\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_107880\\2367586196.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m         store_transition(state, action, reward, next_state, terminated) \u001b[38;5;66;03m# Stockage de la transition dans la mémoire\u001b[39;00m\n\u001b[32m     13\u001b[39m         total_reward += reward \u001b[38;5;66;03m# Ajout de la récompense totale\u001b[39;00m\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m         state = next_state \u001b[38;5;66;03m# Mise à jour de l'état\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         train_step() \u001b[38;5;66;03m# Entraînement du modèle\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m     epsilon = max(epsilon_min, epsilon * epsilon_decay) \u001b[38;5;66;03m# Décroissance d'epsilon (réduction de l'exploration)\u001b[39;00m\n\u001b[32m     19\u001b[39m \n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_107880\\2864440064.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, act \u001b[38;5;28;01min\u001b[39;00m enumerate(action):\n\u001b[32m     15\u001b[39m         target_q_values[i][act] = reward[i] \u001b[38;5;28;01mif\u001b[39;00m terminated[i] \u001b[38;5;28;01melse\u001b[39;00m reward[i] + gamma * max_next_q_values[i]\n\u001b[32m     16\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     q_model.fit(state, target_q_values, epochs=\u001b[32m1\u001b[39m, verbose=\u001b[32m0\u001b[39m, callbacks=callbacks) \u001b[38;5;66;03m# Entraînement du modèle sur l'état actuel et les valeurs Q cibles\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m             \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m             \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m    122\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    351\u001b[39m         epoch_iterator.reset()\n\u001b[32m    352\u001b[39m \n\u001b[32m    353\u001b[39m         \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[32m    354\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(callbacks, callbacks_module.CallbackList):\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m             callbacks = callbacks_module.CallbackList(\n\u001b[32m    356\u001b[39m                 callbacks,\n\u001b[32m    357\u001b[39m                 add_history=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    358\u001b[39m                 add_progbar=verbose != \u001b[32m0\u001b[39m,\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, callbacks, add_history, add_progbar, model, **params)\u001b[39m\n\u001b[32m     45\u001b[39m         self._async_predict = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     46\u001b[39m         self._futures = []\n\u001b[32m     47\u001b[39m         self._configure_async_dispatch(callbacks)\n\u001b[32m     48\u001b[39m         self._add_default_callbacks(add_history, add_progbar)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         self.set_model(model)\n\u001b[32m     50\u001b[39m         self.set_params(params)\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    110\u001b[39m         super().set_model(model)\n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self._history:\n\u001b[32m    112\u001b[39m             model.history = self._history\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;28;01min\u001b[39;00m self.callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m             callback.set_model(model)\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\callbacks\\tensorboard.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    217\u001b[39m         self._writers = {}  \u001b[38;5;66;03m# Resets writers.\u001b[39;00m\n\u001b[32m    218\u001b[39m \n\u001b[32m    219\u001b[39m         self._should_write_train_graph = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.write_graph:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m             self._write_keras_model_summary()\n\u001b[32m    222\u001b[39m             self._should_write_train_graph = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    223\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.embeddings_freq:\n\u001b[32m    224\u001b[39m             self._configure_embeddings()\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\callbacks\\tensorboard.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    268\u001b[39m             if (\n\u001b[32m    269\u001b[39m                 self.model.__class__.__name__ == \u001b[33m\"Functional\"\u001b[39m\n\u001b[32m    270\u001b[39m                 \u001b[38;5;28;01mor\u001b[39;00m self.model.__class__.__name__ == \u001b[33m\"Sequential\"\u001b[39m\n\u001b[32m    271\u001b[39m             ):\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m                 keras_model_summary(\u001b[33m\"keras\"\u001b[39m, self.model, step=\u001b[32m0\u001b[39m)\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\callbacks\\tensorboard.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(name, data, step)\u001b[39m\n\u001b[32m    674\u001b[39m     summary_metadata.plugin_data.content = \u001b[33mb\"1\"\u001b[39m\n\u001b[32m    675\u001b[39m \n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    677\u001b[39m         json_string = data.to_json()\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    679\u001b[39m         \u001b[38;5;66;03m# An exception should not break a model code.\u001b[39;00m\n\u001b[32m    680\u001b[39m         warnings.warn(f\"Model failed to serialize as JSON. Ignoring... {exc}\")\n\u001b[32m    681\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\models\\model.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    520\u001b[39m             A JSON string.\n\u001b[32m    521\u001b[39m         \"\"\"\n\u001b[32m    522\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m keras.src.saving \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n\u001b[32m    523\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m         model_config = serialization_lib.serialize_keras_object(self)\n\u001b[32m    525\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m json.dumps(model_config, **kwargs)\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    261\u001b[39m         obj.__class__, inner_config\n\u001b[32m    262\u001b[39m     )\n\u001b[32m    263\u001b[39m \n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_with_public_class \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m         get_build_and_compile_config(obj, config_with_public_class)\n\u001b[32m    266\u001b[39m         record_object_after_serialization(obj, config_with_public_class)\n\u001b[32m    267\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m config_with_public_class\n\u001b[32m    268\u001b[39m \n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(obj, config)\u001b[39m\n\u001b[32m    299\u001b[39m         build_config = obj.get_build_config()\n\u001b[32m    300\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m build_config \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    301\u001b[39m             config[\u001b[33m\"build_config\"\u001b[39m] = serialize_dict(build_config)\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hasattr(obj, \u001b[33m\"get_compile_config\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m         compile_config = obj.get_compile_config()\n\u001b[32m    304\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m compile_config \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    305\u001b[39m             config[\u001b[33m\"compile_config\"\u001b[39m] = serialize_dict(compile_config)\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    943\u001b[39m         Returns:\n\u001b[32m    944\u001b[39m             A dict containing information \u001b[38;5;28;01mfor\u001b[39;00m compiling the model.\n\u001b[32m    945\u001b[39m         \"\"\"\n\u001b[32m    946\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.compiled \u001b[38;5;28;01mand\u001b[39;00m hasattr(self, \u001b[33m\"_compile_config\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._compile_config.serialize()\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m serialize(self):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m serialize_keras_object(self.config)\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m isinstance(obj, (list, tuple)):\n\u001b[32m    160\u001b[39m         config_arr = [serialize_keras_object(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m obj]\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tuple(config_arr) \u001b[38;5;28;01mif\u001b[39;00m isinstance(obj, tuple) \u001b[38;5;28;01melse\u001b[39;00m config_arr\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m isinstance(obj, dict):\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m serialize_dict(obj)\n\u001b[32m    164\u001b[39m \n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Special cases:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m isinstance(obj, bytes):\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m serialize_dict(obj):\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: serialize_keras_object(value) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;28;01min\u001b[39;00m obj.items()}\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    255\u001b[39m             \u001b[33m\"config\"\u001b[39m: ts_config,\n\u001b[32m    256\u001b[39m             \u001b[33m\"registered_name\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    257\u001b[39m         }\n\u001b[32m    258\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     inner_config = _get_class_or_fn_config(obj)\n\u001b[32m    260\u001b[39m     config_with_public_class = serialize_with_public_class(\n\u001b[32m    261\u001b[39m         obj.__class__, inner_config\n\u001b[32m    262\u001b[39m     )\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m isinstance(obj, types.FunctionType):\n\u001b[32m    384\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object_registration.get_registered_name(obj)\n\u001b[32m    385\u001b[39m     \u001b[38;5;66;03m# All classes:\u001b[39;00m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hasattr(obj, \u001b[33m\"get_config\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m         config = obj.get_config()\n\u001b[32m    388\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(config, dict):\n\u001b[32m    389\u001b[39m             raise TypeError(\n\u001b[32m    390\u001b[39m                 f\"The `get_config()` method of {obj} should return \"\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\adam.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m get_config(self):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m         config = super().get_config()\n\u001b[32m    141\u001b[39m         config.update(\n\u001b[32m    142\u001b[39m             {\n\u001b[32m    143\u001b[39m                 \u001b[33m\"beta_1\"\u001b[39m: self.beta_1,\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1034\u001b[39m             learning_rate = learning_rate_schedule.serialize(\n\u001b[32m   1035\u001b[39m                 self._learning_rate\n\u001b[32m   1036\u001b[39m             )\n\u001b[32m   1037\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m isinstance(self._learning_rate, backend.Variable):\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m             learning_rate = float(self._learning_rate.numpy())\n\u001b[32m   1039\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m ops.is_tensor(self._learning_rate):\n\u001b[32m   1040\u001b[39m             learning_rate = float(self._learning_rate)\n\u001b[32m   1041\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m callable(self._learning_rate):\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m numpy(self):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.value.numpy()\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    711\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m numpy(self):\n\u001b[32m    712\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m self.read_value().numpy()\n\u001b[32m    714\u001b[39m     raise NotImplementedError(\n\u001b[32m    715\u001b[39m         \u001b[33m\"numpy() is only available when eager execution is enabled.\"\u001b[39m)\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(\u001b[33m\"Read\"\u001b[39m):\n\u001b[32m    881\u001b[39m       value = self._read_variable_op()\n\u001b[32m    882\u001b[39m     \u001b[38;5;66;03m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[32m    883\u001b[39m     \u001b[38;5;66;03m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m884\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m array_ops.identity(value)\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m op(*args, **kwargs)\n\u001b[32m     89\u001b[39m     bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m     90\u001b[39m     bound_arguments.apply_defaults()\n\u001b[32m     91\u001b[39m     bound_kwargs = bound_arguments.arguments\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(input, name)\u001b[39m\n\u001b[32m    306\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m context.executing_eagerly() \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m hasattr(input, \u001b[33m\"graph\"\u001b[39m):\n\u001b[32m    307\u001b[39m     \u001b[38;5;66;03m# Make sure we get an input with handle data attached from the resource\u001b[39;00m\n\u001b[32m    308\u001b[39m     \u001b[38;5;66;03m# variables. Variables have correct handle data when graph building.\u001b[39;00m\n\u001b[32m    309\u001b[39m     input = ops.convert_to_tensor(input)\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m   ret = gen_array_ops.identity(input, name=name)\n\u001b[32m    311\u001b[39m   \u001b[38;5;66;03m# Propagate handles data for happier shape inference for resource variables.\u001b[39;00m\n\u001b[32m    312\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m hasattr(input, \u001b[33m\"_handle_data\"\u001b[39m):\n\u001b[32m    313\u001b[39m     ret._handle_data = input._handle_data  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\Oliwer\\Documents\\deep_learning\\venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(input, name)\u001b[39m\n\u001b[32m   4227\u001b[39m         _ctx, \u001b[33m\"Identity\"\u001b[39m, name, input)\n\u001b[32m   4228\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   4229\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   4230\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m-> \u001b[39m\u001b[32m4231\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m   4232\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   4233\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   4234\u001b[39m       return identity_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Itération des entrainements\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    terminated = False\n",
    "    \n",
    "    while not terminated:\n",
    "        action = epsilon_greedy_policy(state, epsilon) # Sélection de l'action avec la politique epsilon-greedy\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action) # Modifier l'environnement avec l'action\n",
    "        terminated = terminated or truncated # Vérification de la terminaison de l'épisode\n",
    "        store_transition(state, action, reward, next_state, terminated) # Stockage de la transition dans la mémoire\n",
    "        total_reward += reward # Ajout de la récompense totale\n",
    "\n",
    "        state = next_state # Mise à jour de l'état\n",
    "        train_step() # Entraînement du modèle\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay) # Décroissance d'epsilon (réduction de l'exploration)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        target_model.set_weights(q_model.get_weights()) # Mise à jour du modèle cible avec les poids du modèle principal\n",
    "    \n",
    "    reward_history.append(total_reward) # Ajout de la récompense totale à l'historique des récompenses\n",
    "    print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1b332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du modèle\n",
    "target_model.save('CartPole_model.keras') # Sauvegarde du modèle cible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
